{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза <<мешка слов>>. Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать <<мешком ингредиентов>>, потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные <<темы>>. Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули json и gensim. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "pip install gensim\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (\"cuisine\") и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'cuisine': u'southern_us', u'id': 25693, u'ingredients': [u'plain flour', u'ground pepper', u'salt', u'tomatoes', u'ground black pepper', u'thyme', u'eggs', u'green tomatoes', u'yellow corn meal', u'milk', u'vegetable oil']}\n"
     ]
    }
   ],
   "source": [
    "print recipes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая и влезает в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'romaine lettuce', u'black olives', u'grape tomatoes', u'garlic', u'pepper', u'purple onion', u'seasoning', u'garbanzo beans', u'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print texts[0]\n",
    "print corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть две полезных переменных: dictionary.id2token и dictionary.token2id; эти словари позволяют находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. Затем вызовите метод модели show_topics, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода show_topics указать параметр formatted=True, то топы ингредиентов будет удобно выводить на печать, если formatted=False, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 3.07 s, total: 2min 12s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.089*cooking spray + 0.083*salt + 0.080*garlic cloves + 0.068*olive oil + 0.066*chopped onion + 0.037*crushed red pepper + 0.036*fat free less sodium chicken broth + 0.034*black pepper + 0.032*ground black pepper + 0.032*water'),\n",
       " (1,\n",
       "  u'0.085*carrots + 0.059*onions + 0.057*sauce + 0.056*cabbage + 0.046*spinach + 0.039*beef + 0.033*low sodium chicken broth + 0.033*water + 0.029*firm tofu + 0.028*green cabbage'),\n",
       " (2,\n",
       "  u'0.066*cracked black pepper + 0.059*dry red wine + 0.041*shortening + 0.041*beef broth + 0.040*vegetable oil cooking spray + 0.039*grape tomatoes + 0.035*cilantro sprigs + 0.031*french bread + 0.029*dried rosemary + 0.029*all-purpose flour'),\n",
       " (3,\n",
       "  u'0.133*potatoes + 0.124*oil + 0.063*salt + 0.059*chickpeas + 0.042*onions + 0.038*coriander + 0.034*pepper + 0.033*saffron + 0.031*chopped tomatoes + 0.026*vegetables'),\n",
       " (4,\n",
       "  u'0.062*green bell pepper + 0.058*garlic powder + 0.056*cayenne pepper + 0.053*salt + 0.044*paprika + 0.035*onions + 0.035*dried thyme + 0.032*worcestershire sauce + 0.027*onion powder + 0.027*ground black pepper'),\n",
       " (5,\n",
       "  u'0.077*ground cumin + 0.052*salt + 0.048*ground coriander + 0.036*onions + 0.036*olive oil + 0.033*garlic + 0.028*paprika + 0.026*tumeric + 0.023*cayenne pepper + 0.022*garlic cloves'),\n",
       " (6,\n",
       "  u'0.107*ground cinnamon + 0.087*ground nutmeg + 0.056*honey + 0.047*ground allspice + 0.046*raisins + 0.046*ground cloves + 0.044*light brown sugar + 0.031*ground ginger + 0.030*brown sugar + 0.030*margarine'),\n",
       " (7,\n",
       "  u'0.085*zucchini + 0.080*plum tomatoes + 0.077*fresh basil + 0.074*olive oil + 0.047*eggplant + 0.039*salt + 0.031*grated parmesan cheese + 0.030*garlic cloves + 0.030*red bell pepper + 0.029*spaghetti'),\n",
       " (8,\n",
       "  u'0.087*rice + 0.077*cooking oil + 0.067*salt + 0.059*water + 0.051*basmati rice + 0.049*vinegar + 0.046*ginger + 0.036*curry leaves + 0.029*mint leaves + 0.023*red cabbage'),\n",
       " (9,\n",
       "  u'0.079*dried oregano + 0.073*onions + 0.061*garlic + 0.057*tomato sauce + 0.053*salt + 0.050*ground beef + 0.049*diced tomatoes + 0.043*dried basil + 0.040*tomato paste + 0.035*olive oil'),\n",
       " (10,\n",
       "  u'0.049*white wine + 0.048*ground black pepper + 0.037*butter + 0.036*russet potatoes + 0.031*chopped fresh chives + 0.030*kosher salt + 0.027*red potato + 0.027*ham + 0.026*large eggs + 0.024*salt'),\n",
       " (11,\n",
       "  u'0.067*jalapeno chilies + 0.064*salt + 0.051*avocado + 0.042*lime + 0.035*purple onion + 0.034*garlic + 0.032*olive oil + 0.032*fresh cilantro + 0.031*cilantro + 0.029*lime juice'),\n",
       " (12,\n",
       "  u'0.134*cucumber + 0.070*lean ground beef + 0.046*cider vinegar + 0.045*feta cheese + 0.040*lemon wedge + 0.039*romaine lettuce + 0.035*cream + 0.034*cherry tomatoes + 0.031*chili + 0.027*taco seasoning mix'),\n",
       " (13,\n",
       "  u'0.086*sour cream + 0.056*salsa + 0.049*shredded cheddar cheese + 0.048*flour tortillas + 0.041*chili powder + 0.039*black beans + 0.034*green onions + 0.031*ground cumin + 0.026*corn tortillas + 0.024*cheddar cheese'),\n",
       " (14,\n",
       "  u'0.111*white sugar + 0.086*sweet potatoes + 0.058*fresh mint + 0.050*black peppercorns + 0.045*fennel seeds + 0.037*red wine + 0.032*vegetable stock + 0.024*sugar + 0.023*maple syrup + 0.022*garlic chili sauce'),\n",
       " (15,\n",
       "  u'0.090*cold water + 0.072*cinnamon sticks + 0.057*boiling water + 0.044*sugar + 0.041*star anise + 0.033*slivered almonds + 0.029*cake flour + 0.027*panko breadcrumbs + 0.026*green olives + 0.024*apple cider vinegar'),\n",
       " (16,\n",
       "  u'0.101*extra-virgin olive oil + 0.060*garlic cloves + 0.050*olive oil + 0.046*flat leaf parsley + 0.039*freshly ground pepper + 0.039*fresh lemon juice + 0.038*salt + 0.036*dry white wine + 0.032*large garlic cloves + 0.029*ground black pepper'),\n",
       " (17,\n",
       "  u'0.140*coconut milk + 0.054*parsley + 0.048*thyme + 0.045*chicken thighs + 0.035*garlic + 0.033*coconut oil + 0.033*onions + 0.025*Thai red curry paste + 0.024*bread + 0.020*salt'),\n",
       " (18,\n",
       "  u'0.103*salt + 0.090*all-purpose flour + 0.087*eggs + 0.085*milk + 0.070*butter + 0.055*baking powder + 0.050*sugar + 0.040*flour + 0.034*baking soda + 0.030*buttermilk'),\n",
       " (19,\n",
       "  u'0.135*curry powder + 0.071*frozen peas + 0.065*long-grain rice + 0.053*sweetened condensed milk + 0.046*greek yogurt + 0.038*egg whites + 0.032*cauliflower + 0.030*cardamom pods + 0.028*black-eyed peas + 0.026*ground cayenne pepper'),\n",
       " (20,\n",
       "  u'0.044*peanut oil + 0.042*ground white pepper + 0.035*beansprouts + 0.035*Sriracha + 0.032*rice noodles + 0.032*medium shrimp + 0.030*peanuts + 0.025*minced ginger + 0.025*fish sauce + 0.024*english cucumber'),\n",
       " (21,\n",
       "  u'0.116*boneless skinless chicken breast halves + 0.107*coarse salt + 0.067*ground pepper + 0.060*sweet onion + 0.055*mayonaise + 0.042*pork tenderloin + 0.040*minced garlic + 0.038*juice + 0.035*minced onion + 0.031*dried parsley'),\n",
       " (22,\n",
       "  u'0.080*diced onions + 0.054*lettuce + 0.054*self rising flour + 0.048*provolone cheese + 0.032*iceberg lettuce + 0.028*frozen pastry puff sheets + 0.026*romano cheese + 0.025*semi-sweet chocolate morsels + 0.024*shredded cheese + 0.024*reduced-fat sour cream'),\n",
       " (23,\n",
       "  u'0.186*lemon juice + 0.092*dijon mustard + 0.051*creole seasoning + 0.040*white pepper + 0.039*fresh orange juice + 0.038*nutmeg + 0.029*kale + 0.028*mayonaise + 0.028*salmon fillets + 0.027*cream cheese, soften'),\n",
       " (24,\n",
       "  u'0.082*sugar + 0.072*whipping cream + 0.057*orange juice + 0.054*hot water + 0.050*chopped garlic + 0.049*orange + 0.033*bananas + 0.030*bread flour + 0.029*water + 0.029*brandy'),\n",
       " (25,\n",
       "  u'0.066*salt + 0.063*cumin seed + 0.054*onions + 0.045*ground turmeric + 0.041*garam masala + 0.041*green chilies + 0.040*clove + 0.031*chili powder + 0.031*tomatoes + 0.028*oil'),\n",
       " (26,\n",
       "  u'0.084*fish sauce + 0.046*white vinegar + 0.042*sugar + 0.037*garlic + 0.036*lime juice + 0.035*vegetable oil + 0.034*shallots + 0.030*water + 0.030*lemongrass + 0.027*red chili peppers'),\n",
       " (27,\n",
       "  u'0.103*grated parmesan cheese + 0.056*warm water + 0.056*salt + 0.046*shredded mozzarella cheese + 0.043*olive oil + 0.038*ricotta cheese + 0.037*active dry yeast + 0.036*mozzarella cheese + 0.032*butter + 0.031*italian seasoning'),\n",
       " (28,\n",
       "  u'0.199*shrimp + 0.057*pork + 0.055*baby spinach + 0.034*jasmine rice + 0.033*unsweetened coconut milk + 0.033*fresh tomatoes + 0.032*noodles + 0.031*black mustard seeds + 0.028*sausage casings + 0.025*sea scallops'),\n",
       " (29,\n",
       "  u'0.134*mushrooms + 0.082*white wine vinegar + 0.048*shallots + 0.042*boneless chicken breast + 0.034*olive oil + 0.033*chopped fresh sage + 0.027*butter + 0.026*chorizo sausage + 0.026*blanched almonds + 0.024*chicken drumsticks'),\n",
       " (30,\n",
       "  u'0.089*large eggs + 0.088*unsalted butter + 0.067*sugar + 0.057*salt + 0.054*all-purpose flour + 0.050*heavy cream + 0.038*whole milk + 0.033*vanilla extract + 0.033*granulated sugar + 0.029*large egg yolks'),\n",
       " (31,\n",
       "  u'0.071*salt + 0.063*ground red pepper + 0.052*water + 0.052*finely chopped onion + 0.046*chopped celery + 0.032*green beans + 0.031*hot pepper sauce + 0.031*long grain white rice + 0.030*chopped onion + 0.028*chopped green bell pepper'),\n",
       " (32,\n",
       "  u'0.063*water + 0.060*fine sea salt + 0.050*lemon zest + 0.049*fresh spinach + 0.041*yukon gold potatoes + 0.036*sesame seeds + 0.034*collard greens + 0.032*parmigiano reggiano cheese + 0.031*rice flour + 0.029*tofu'),\n",
       " (33,\n",
       "  u'0.133*balsamic vinegar + 0.070*dark soy sauce + 0.066*chopped fresh mint + 0.063*baguette + 0.051*broccoli + 0.031*penne + 0.031*mint + 0.028*tequila + 0.019*red kidney beans + 0.018*arugula'),\n",
       " (34,\n",
       "  u'0.089*onions + 0.074*salt + 0.059*garlic + 0.049*olive oil + 0.041*bay leaves + 0.041*ground black pepper + 0.038*chicken stock + 0.037*pepper + 0.037*carrots + 0.032*chicken'),\n",
       " (35,\n",
       "  u'0.097*soy sauce + 0.052*sesame oil + 0.042*sugar + 0.042*corn starch + 0.039*garlic + 0.035*green onions + 0.035*scallions + 0.034*rice vinegar + 0.029*salt + 0.028*water'),\n",
       " (36,\n",
       "  u'0.109*cheese + 0.064*garlic salt + 0.061*rice wine + 0.058*chives + 0.043*chicken breast halves + 0.041*shredded sharp cheddar cheese + 0.040*roasted peanuts + 0.039*Mexican cheese blend + 0.037*non-fat sour cream + 0.034*dashi'),\n",
       " (37,\n",
       "  u'0.180*chicken broth + 0.101*chicken breasts + 0.062*onions + 0.053*crushed red pepper flakes + 0.053*garlic + 0.041*pepper + 0.038*chopped parsley + 0.035*salt + 0.034*olive oil + 0.025*bay leaf'),\n",
       " (38,\n",
       "  u'0.064*olive oil + 0.060*red wine vinegar + 0.054*salt + 0.045*parmesan cheese + 0.041*garlic + 0.040*fresh basil leaves + 0.037*extra-virgin olive oil + 0.031*pepper + 0.030*ground black pepper + 0.030*tomatoes'),\n",
       " (39,\n",
       "  u'0.101*canola oil + 0.066*fresh lime juice + 0.063*chopped cilantro fresh + 0.058*kosher salt + 0.054*garlic cloves + 0.034*peeled fresh ginger + 0.030*chiles + 0.029*vegetable oil + 0.028*serrano chile + 0.026*boneless chicken skinless thigh')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topics(num_topics=40, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 7 8 1 1 1\n"
     ]
    }
   ],
   "source": [
    "tops = ldamodel.show_topics(num_topics=40, num_words=10, formatted=False)\n",
    "c_salt=0 \n",
    "c_sugar=0\n",
    "c_water=0\n",
    "c_mushrooms=0\n",
    "c_chicken=0\n",
    "c_eggs=0\n",
    "for i in range(40):\n",
    "    pairs = tops[i][1]\n",
    "    for p in pairs:\n",
    "        word = p[0]\n",
    "        if word==\"salt\":\n",
    "            c_salt+=1\n",
    "        elif word==\"sugar\":\n",
    "            c_sugar+=1\n",
    "        elif word==\"water\":\n",
    "            c_water+=1\n",
    "        elif word==\"mushrooms\":\n",
    "            c_mushrooms+=1\n",
    "        elif word==\"chicken\":\n",
    "            c_chicken+=1\n",
    "        elif word==\"eggs\":\n",
    "            c_eggs+=1\n",
    "            \n",
    "print c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами - фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная dfs - это словарь, ключами которого являются id токена, а элементами - число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря filter_tokens, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after - размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after - суммарное количество ингредиентов в корпусе до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 9, 12, 17, 21, 29, 45, 48, 54, 100, 117]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_ids = [d for d in dictionary2.dfs if dictionary2.dfs[d]>4000]\n",
    "popular_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary2.dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714 6702\n"
     ]
    }
   ],
   "source": [
    "dict_size_before = len(dictionary)\n",
    "dictionary2.filter_tokens(popular_ids)\n",
    "dict_size_after = len(dictionary2)\n",
    "print dict_size_before, dict_size_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428249 343665\n"
     ]
    }
   ],
   "source": [
    "corpus_size_before = 0\n",
    "for rec in corpus:\n",
    "    corpus_size_before+=len(rec)\n",
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]\n",
    "corpus_size_after=0\n",
    "for rec in corpus2:\n",
    "    corpus_size_after+=len(rec)\n",
    "#corpus_size_after = len(corpus2)\n",
    "print corpus_size_before, corpus_size_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))\n",
    "\n",
    "save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictioanary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом top_topics модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 76.3 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel2 = models.ldamodel.LdaModel(corpus2, id2word=dictionary2, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tops1 = ldamodel.top_topics(corpus)\n",
    "tops2 = ldamodel2.top_topics(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-627.26151034 -682.552673193\n"
     ]
    }
   ],
   "source": [
    "def get_avg_coherence(top_topics):\n",
    "    coh_arr1 = [x[1] for x in top_topics]\n",
    "    return np.average(coh_arr1)\n",
    "\n",
    "coherence = get_avg_coherence(tops1)\n",
    "coherence2 = get_avg_coherence(tops2)\n",
    "print coherence, coherence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))\n",
    "save_answers3(coherence, coherence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом get_document_topics второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.12812499999999991),\n",
       " (15, 0.32353084838300322),\n",
       " (22, 0.27661108248543986),\n",
       " (37, 0.15923306913155641)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_top = ldamodel2.get_document_topics(corpus2[0])\n",
    "doc_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной .alpha второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,\n",
       "        0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,\n",
       "        0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,\n",
       "        0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,\n",
       "        0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025,  0.025])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel2.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром minimum_probability=0.01 и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 114 ms, total: 1min 28s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel3 = models.ldamodel.LdaModel(corpus2, id2word=dictionary2, num_topics=40, passes=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.021286343792953501), (1, 0.021298590310099758), (2, 0.021320055508571786), (3, 0.021282109727077194), (4, 0.021276595744680844), (5, 0.021496606933716404), (6, 0.021292176774168874), (7, 0.04255970642916266), (8, 0.02127663405745726), (9, 0.021276595744680844), (10, 0.021574647158752052), (11, 0.021287570547940532), (12, 0.021277424425023877), (13, 0.084096705567749952), (14, 0.021290106135531339), (15, 0.021695610486780825), (16, 0.021284453838081592), (17, 0.0212800707131021), (18, 0.021340039091641363), (19, 0.021278463140557358), (20, 0.021276595744752807), (21, 0.021334086322766982), (22, 0.021281172160394983), (23, 0.021394919949490766), (24, 0.021288024651372802), (25, 0.062709262165435403), (26, 0.021292249798166067), (27, 0.021292331136060234), (28, 0.021307418247128996), (29, 0.021434803424634722), (30, 0.021432324163326053), (31, 0.021282086472031888), (32, 0.021325744739054308), (33, 0.042881410740644103), (34, 0.021276595744680955), (35, 0.021282060707513067), (36, 0.021303897740526021), (37, 0.021276595744680844), (38, 0.021276841759480063), (39, 0.021281072460128531)]\n"
     ]
    }
   ],
   "source": [
    "print ldamodel3.get_document_topics(corpus2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198898 1590960\n"
     ]
    }
   ],
   "source": [
    "def get_doc_topics_sum(model):\n",
    "    s = 0\n",
    "    for doc in corpus2:\n",
    "        topics = model.get_document_topics(doc, minimum_probability=0.01)\n",
    "        s+=len(topics)\n",
    "    return s\n",
    "\n",
    "count_model2 = get_doc_topics_sum(ldamodel2)\n",
    "count_model3 = get_doc_topics_sum(ldamodel3)\n",
    "print count_model2, count_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))\n",
    "save_answers4(count_model2, count_model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр alpha влияет на разреженность распределений тем в документах. Аналогично гиперпараметр eta влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774 39774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "print len(corpus2), len(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [r[\"cuisine\"] for r in recipes]\n",
    "X = np.zeros((len(recipes), 40))\n",
    "for i in range(len(recipes)):\n",
    "    topics = ldamodel2.get_document_topics(corpus2[i])\n",
    "    for topic in topics:\n",
    "        idx = topic[0]\n",
    "        val = topic[1]\n",
    "        X[i,idx]=val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56103446  0.55886568  0.57123453]\n",
      "0.563711554514\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "score = cross_val_score(rf,X,y)\n",
    "print score\n",
    "accuracy = np.mean(score)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))\n",
    "save_answers5(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10-15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA --- вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(0, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу A размера темы x кухни, ее элементы $a_{tc}$ - суммы p(t|d) по всем документам d, которые отнесены к кухне c. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу A. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
